apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: prometheus-stack
  namespace: observability
spec:
  interval: 30m
  timeout: 15m
  chart:
    spec:
      chart: kube-prometheus-stack
      version: 69.3.0
      sourceRef:
        kind: HelmRepository
        name: prometheus-community
        namespace: flux-system
  maxHistory: 2
  install:
    createNamespace: true
    crds: CreateReplace
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    crds: CreateReplace
    remediation:
      retries: 3
  uninstall:
    keepHistory: false
  values:
    crds:
      enabled: true
    defaultRules:
      create: false   #this creates alert manager rules
      rules:
        alertmanager:                       true
        etcd:                               false
        configReloaders:                    true
        general:                            true
        k8sContainerCpuUsageSecondsTotal:   true
        k8sContainerMemoryCache:            true
        k8sContainerMemoryRss:              true
        k8sContainerMemorySwap:             true
        k8sContainerResource:               true
        k8sContainerMemoryWorkingSetBytes:  true
        k8sPodOwner:                        true
        kubeApiserverAvailability:          true
        kubeApiserverBurnrate:              true
        kubeApiserverHistogram:             true
        kubeApiserverSlos:                  true
        kubeControllerManager:              false
        kubelet:                            true
        kubeProxy:                          false
        kubePrometheusGeneral:              true
        kubePrometheusNodeRecording:        true
        kubernetesApps:                     true
        kubernetesResources:                true
        kubernetesStorage:                  true
        kubernetesSystem:                   true
        kubeSchedulerAlerting:              false
        kubeSchedulerRecording:             true
        kubeStateMetrics:                   true
        network:                            true
        node:                               true
        nodeExporterAlerting:               true
        nodeExporterRecording:              true
        prometheus:                         true
        prometheusOperator:                 true
        windows:                            true
    ## Setting to true produces cleaner resource names, but requires a data migration because the name of the persistent volume changes. Therefore this should only be set once on initial installation.
    ##
    cleanPrometheusOperatorObjectNames: true

    ## Extra manifests to deploy as an array
    extraManifests: []

    # kubeApiServer:
    #   enabled: false
    # kubelet:
    #   enabled: false
    kubeControllerManager:
      enabled: false
    # coreDns:
    #   enabled: false
    # kubeDns:
    #   enabled: false
    # kubeEtcd:
    #   enabled: false
    # kubeScheduler:
    #   enabled: false
    kubeProxy:
      enabled: false
    # kubeStateMetrics:
    #   enabled: false
    # ## Deploy node exporter as a daemonset to all nodes
    # nodeExporter:
    #   enabled: false
    # prometheusOperator:
    #   enabled: true
    #   services:
    #     type: "LoadBalancer"
    prometheus:
      enabled: true
      ingress:
        enabled: true
        ingressClassName: traefik-external
        hosts:
          - prometheus.cloud.icylair.com
        annotations:
          traefik.ingress.kubernetes.io/router.middlewares: network-forward-auth@kubernetescrd
        tls:
          - secretName: icylair-com-all-prod
            hosts:
              - prometheus.cloud.icylair.com
      prometheusSpec:
        podMonitorSelectorNilUsesHelmValues: false
        probeSelectorNilUsesHelmValues: false
        ruleSelectorNilUsesHelmValues: false
        scrapeConfigSelectorNilUsesHelmValues: false
        serviceMonitorSelectorNilUsesHelmValues: false
        enableAdminAPI: true
        walCompression: true
        enableFeatures:
          - auto-gomemlimit
          - memory-snapshot-on-shutdown
          - new-service-discovery-manager
          - exemplar-storage
        retention: 14d
        retentionSize: 8GB
        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: longhorn-storage
              accessModes: ["ReadWriteOnce"]
              resources:
                requests:
                  storage: 10Gi
            selector: {}
        # additionalScrapeConfigs: | # this shit breaks the prometherus pod dynamicaly
        #   - job_name: 'kubernetes-pods'
        #     kubernetes_sd_configs:
        #     - role: pod
        #     relabel_configs:
        #       - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        #         action: keep
        #         regex: true
        #   - job_name: 'kubernetes-endpoints'
        #     scrape_interval: 30s
        #     kubernetes_sd_configs:
        #       - role: endpoints
        #     relabel_configs:
        #       - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
        #         action: keep
        #         regex: true
      service:
        type: "LoadBalancer"
    grafana:
      enabled: false
      forceDeployDashboards: true
    alertmanager:
      enabled: true
      service:
        annotations: {}
        labels: {}
        type: LoadBalancer
        loadBalancerSourceRanges: # prob will brake stuff if i change lb, can be removed not necesarry 
          - "10.49.20.0/24"
        # ipDualStack:
        #   enabled: false
        #   ipFamilies: ["IPv6", "IPv4"]
        #   ipFamilyPolicy: "PreferDualStack"
      ingress:
        enabled: true
        className: traefik-external
        hosts:
          - &host alertmanager.cloud.icylair.com
        annotations:
          traefik.ingress.kubernetes.io/router.middlewares: network-forward-auth@kubernetescrd
        tls:
          - secretName: icylair-com-all-prod
            hosts:
              - *host
        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: longhorn-storage
              accessModes: ["ReadWriteOnce"]
              resources:
                requests:
                  storage: 500Mi
            selector: {}
      resources:
        cpu: 10m
        memory: 32Mi
      alertmanagerSpec:
        useExistingSecret: true
        configSecret: alertmanager-secret
      # config:
      #   # enabled: true
      #   global: {}
      #   #   webhook_url: ''
      #   route:
      #     group_by: ['alertname', 'job']
      #     group_wait: 30s
      #     group_interval: 5m
      #     repeat_interval: 3h
      #     receiver: discord
      #   receivers:
      #   - name: discord
      #     discord_configs:
      #     - webhook_url: 
      # # templates:
      # #   # alertmanager.yml: |
      # #   #   route:
      # #   #     group_by: ['alertname', 'job']
      # #   #     group_wait: 30s
      # #   #     group_interval: 5m
      # #   #     repeat_interval: 3h
      # #   #     receiver: discord
      # #   #   receivers:
      # #   #   - name: discord
      # #   #     discord_configs:
      # #   #     - webhook_url: 



    additionalPrometheusRulesMap:
      demo-alerts.yml:
        groups:
        - name: demo-service-alerts
          rules:
          - alert: DemoServiceHighErrorRate
            expr: |
              (
                sum without(status, instance) (
                  rate(demo_api_request_duration_seconds_count{status=~"5..",job="demo"}[1m])
                )
              /
                sum without(status, instance) (
                  rate(demo_api_request_duration_seconds_count{job="demo"}[1m])
                ) * 100 > 0.5
              )
            for: 1m
            labels:
              severity: critical
            annotations:
              title: 'High 5xx rate for {{ $labels.method }} on {{ $labels.path }}'
              description: 'The 5xx error rate for path {{$labels.path}} with method {{ $labels.method }} in {{ $labels.job }} is {{ printf "%.2f" $value }}%.'
    # prometheus.yml:
    #   global:
    #     scrape_interval: 15s
    #     evaluation_interval: 15s
    #   rule_files:
    #     - demo-alerts.yml
    #   alerting:
    #     alertmanagers:
    #     - static_configs:
    #       - targets: ['localhost:9093']
    #   scrape_configs:
    #     - job_name: 'demo'
    #       static_configs:
    #         - targets:
    #           - 'demo.promlabs.com:10000'
    #           - 'demo.promlabs.com:10001'
    #           - 'demo.promlabs.com:10002'