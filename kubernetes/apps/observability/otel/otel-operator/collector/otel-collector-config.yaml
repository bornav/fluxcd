---
# yaml-language-server: $schema=https://raw.githubusercontent.com/open-telemetry/opentelemetry-configuration/refs/heads/main/schema/common.json
apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
metadata:
  name: opentelemetry
spec:
  # service:
  #   loadBalanceIp: "10.10.10.10" # TODO find out how this is done, since as usual otel docs are shit
  tolerations: 
    - key: node-role.kubernetes.io/control-plane
      value: "true"
      effect: NoSchedule
  volumes:
  # - name: pod-logs
  #   hostPath:
  #     path: /var/log/pods
  - name: varlog
    hostPath:
      path: /var/log
  - name: rke2agent
    hostPath:
      path: /var/lib/rancher/rke2/agent/logs/
  # - name: journal
  #   hostPath:
  #     path: /var/log/journal
  volumeMounts:
  # - name: pod-logs
  #   mountPath: /var/log/pods
  #   readOnly: true
  - name: varlog
    mountPath: /var/log
    readOnly: true
  - name: rke2agent
    mountPath: /var/lib/rancher/rke2/agent/logs/
    readOnly: true
  # - name: journal
  #   mountPath: /var/log/journal
  #   readOnly: true
  env:
  - name: K8S_NODE_NAME
    valueFrom:
      fieldRef:
        fieldPath: spec.nodeName
  mode: daemonset
  config:
    receivers:
      filelog:
        include_file_path: true
        include:
          - /var/log/pods/*/*/*.log
        operators:
          - id: container-parser
            type: container
      jaeger:
        protocols:
          grpc:
            endpoint: ${env:MY_POD_IP}:14250
          thrift_compact:
            endpoint: ${env:MY_POD_IP}:6831
          thrift_http:
            endpoint: ${env:MY_POD_IP}:14268
      otlp:
        protocols:
          grpc:
            endpoint: ${env:MY_POD_IP}:4317
          http:
            cors:
              allowed_origins:
                - "http://*"
                - "https://*"
            endpoint: ${env:MY_POD_IP}:4318
    extensions:
      health_check:
        endpoint: ${env:MY_POD_IP}:13133
    exporters:
      debug: {}
      otlp:
        endpoint: jaegertracing-collector.observability.svc.cluster.local:4317
        tls:
          insecure: true
      otlp/grafana_cloud_tempo:
        endpoint: grafana-tempo.observability.svc.cluster.local:4317
        tls:
          insecure: true
      otlphttp:
        endpoint: http://grafana-loki.observability.svc.cluster.local:3100/otlp
        tls:
          insecure: true
    processors:
      batch: {}
      k8sattributes:
        extract:
          metadata: [k8s.namespace.name, k8s.deployment.name, k8s.statefulset.name, k8s.daemonset.name, k8s.cronjob.name, k8s.job.name, k8s.node.name, k8s.pod.name, k8s.pod.uid, k8s.pod.start_time]
        passthrough: false
        pod_association:
        - sources:
          - from: resource_attribute
            name: k8s.pod.ip
        - sources:
          - from: resource_attribute
            name: k8s.pod.uid
        - sources:
      memory_limiter:
        check_interval: 5s
        limit_percentage: 80
        spike_limit_percentage: 25
      resource:
        attributes:
        - action: insert
          from_attribute: k8s.pod.uid
          key: service.instance.id
    connectors:
      spanmetrics: {}
    service:
      extensions:
      - health_check
      pipelines:
        traces:
          receivers: [otlp]
          processors: [batch]
          exporters: [otlp, debug, spanmetrics,otlp/grafana_cloud_tempo]
        metrics:
          receivers: [otlp, spanmetrics]
          processors: [batch, k8sattributes]
          exporters: [debug]
        logs:
          receivers: [otlp,filelog]
          # receivers: [otlp]
          processors: [batch, k8sattributes]
          exporters: [debug, otlphttp]
      telemetry:
        metrics: []
          # address: ${env:MY_POD_IP}:8888